InstructGPT高效实践——【DeepSpeed-Chat】源码详解(3/3)：RLHF Finetuning

https://blog.csdn.net/Remixa/article/details/130666878

目录
前言
3 phase-3: RLHF Finetuning
3.1 训练数据样例
3.1 基本数据
3.2 经验数据
3.2 训练过程
3.3 关键代码详解
3.3.1 读取数据集及Datacollator
3.3.1.1 prompt数据集的读取
3.3.1.2 DataCollatorRLHF
3.3.1.3 无监督数据集的读取
3.3.2 初始化各相关模型
3.3.2.1 模型初始化过程
3.3.2.2 DeepSpeedHybridEngine
3.3.3 根据prompt获取经验数据
3.3.3.1 经验数据获取过程
3.3.3.2 seq的生成
3.3.3.3 奖励reward_score和价值估计values的获取
3.3.3.4 策略模型logits的进一步处理
3.3.4 PPO训练数据管理-MiniDataset
3.3.5 PPO训练过程
3.3.5.1 基本流程
3.3.5.2 PPO训练
3.3.5.3 无监督训练
3.3.5.4 EMA
3.4 实例测试
3.5 相关拓展
3.5.1 phase3的参数设置
3.5.2 PPO-ptx训练的迭代数对齐
3.5.3 LMFlow的RAFT
3.6 版块相关问题
前言
本篇为上中下三篇章的【下篇】，接续自【中篇】。
主要针对整个DeepSpeed-Chat框架中最为复杂的第三阶段进行详解，其中涉及到部分InstructGPT所述相关原理的实践，
基于其代码将更易于理解诸多原理中一笔带过的话题，如“用于经验采集的策略网络到底是SFT还是Actor”“Critic的迭代又是如何实现的”等等。

尽管这是讨论DeepSpeed-Chat技术为主的文章，
但还是不得不借用ColossalChat（另一个实现了RLHF Pipeline的开源项目，项目地址）绘制的流程示意图（下图），
因为它所描绘的第三阶段训练流程，非常详细且与DeepSpeed-Chat的实现过程基本一致，
而DeepSpeed-Chat本身给出的示意图实在太过简略（见【中篇】头图）。
008.png
相信结合这张示意图来学习phase3效果更佳。

3 phase-3: RLHF Finetuning

3.1 训练数据样例

3.1 基本数据

数据格式名称	    说明	                                                                                样例
prompt	        对当前情境的描述，为模型生成提供指令输入信息，可以理解为通俗含义上的“问句”，适用于phase3。	    "Human: Please tell me about Microsoft in a few sentence? Assistant: "

3.2 经验数据
数据格式名称	    说明	样例
prompt	        对当前情境的描述，为模型生成提供指令输入信息，可以理解为通俗含义上的“问句”，适用于phase3。	"Human: Please tell me about Microsoft in a few sentence? Assistant: "（举文本例子是为了便于理解，实际上此处为input_ids）
seq	            actor基于prompt输入生成的完整对话序列。	                                            "Human: Please tell me about Microsoft in a few sentence? Assistant: Microsoft is a world-renowned company."举文本例子是为了便于理解，实际上此处为input_ids）
logprobs	    actor基于seq输出的logits/策略对数。	                                            shape: 本应为(seq_bs, max_seq_len, vocab_size)，经过gather处理后仅取实际label token的log_logit值，为(seq_bs, max_seq_len, 1)。
ref_logprobs	reference/SFT基于seq输出的logits/策略对数。	                                    shape: 本应为(seq_bs, max_seq_len, vocab_size)，经过gather处理后仅取实际label token的log_logit值，为(seq_bs, max_seq_len, 1)。
value	        critic基于seq输出的对序列每个位置的价值评估。	                                    shape: (seq_bs, max_seq_len)
reward	        reward/RM基于seq输出的对整个对话的（环境）奖励。	                                    shape: (seq_bs,)
attention_mask	用于滤掉非有效元素。	                                                            shape: (seq_bs, max_seq_len)

各个框架对于经验数据的定义不完全相同，
例如ColossalChat定义的经验数据还比此处多了项“adv”和“reward”
（此reward非彼reward，ColossalChat的reward指的是“经过KL散度修正后的KL_Reward”），
但本质上都是同理的，只是框定的范围不同，
因为adv（优势函数Adventage）和KL_Reward完全可以由已有项logprobs、ref_logprobs、reward、value计算得到。

从代码效率的角度来考量，ColossalChat的经验数据定义相对更严谨些，
因为优势以及KL惩罚奖励完全可以由基本经验数据计算得到，在生成经验的阶段一步到位计算即可；
而DeepSpeed-Chat中将其安排在训练阶段来计算，每次PPO迭代才计算，优势和KL惩罚奖励是基于基本经验数据计算得到的，
而基本经验数据在生成经验阶段已经确定了，所以即使是在不同的PPO迭代中，优势和KL惩罚奖励也是不变的，
因此DeepSpeed-Chat对adv以及KL惩罚奖励进行了重复计算，这个环节的计算顺序后续（编辑日期2023.05.19）相关团队应该会做出调整。

3.2 训练过程
    在此简单讲述UML时序图的元素含义：
    - 箭头表示信息传递：实线表示调用，虚线表示返回；
    - alt表示假设分支，其后方“[]”中的内容表示“条件”；
    - loop表示循环；
    - 淡蓝色区域即为高亮部分。

....yknote---TBD截图

phase3的大致训练过程如UML时序图所示（“括号序号”与UML时序图的“圈序号”对应）：

1载入tokenizer(1-2)；
2获取Dataset并实例化DataCollator(3-9)：获取用于采集经验的prompt的Dataset(4-5)，
如果启用了无监督训练，则再获取无监督数据的Dataset(6-7)，并且实例化DataCollator用于进一步对加载的数据进行整理；
3实例化DataLoader(10)；
4使用DeepSpeedRLHFEngine()载入PPO训练所需的各个模型（actor、ref/SFT、critic、reward/RM），并进行封装得到rlhf_engine(11-12)；
5实例化PPO的训练管理trainer(13-14)；
6实例化用于PPO训练环节的MiniDataset（有别于上述Dataset，上述Dataset用于获取整个大轮次的数据，
MiniDataset进一步管理Dataset提供的数据，用于分配给PPO轮次、即小轮次进行训练）(15-16)；
7开始训练，大轮次epoch（prompt_epoch）:
    1大轮次step（prompt_step）：
        1使用MiniDataset.add()分配PPO训练轮次所用的训练数据(17-22)：
                1监督数据部分直接使用MiniDataset进行分配(17-18)；
                2经验数据部分将由各相关模型采集得到(19-20)，然后再使用MiniDataset对经验数据进行分配(21-22)；
        2小轮次epoch（ppo_epoch）：
            1小轮次step（ppo_step）：
                1训练得到actor和critic的loss（过程中已经进行过参数更新）(23-24)，
                如果启用了无监督训练，则也进行无监督训练并返回loss(25-26)；
            2当启用指数移动平均技术（ema）时，进行ema(27)；
        3保存模型(28)。
3.3 关键代码详解
上述过程存在几个值得关注的地方（即文字描述加粗、UML时序图高亮的部分）：

    数据集及Datacollator，知悉无监督数据以及prompt数据在读取后、输入模型前的形式；
    各相关模型初始化细节；
    经验数据的获取；
    PPO训练数据管理；
    PPO训练过程。

以下将对相关部分的源码进行讲解。

3.3.1 读取数据集及Datacollator
3.3.1.1 prompt数据集的读取
至于数据集读取，prompt数据集将如【上篇】所述从本地进行读取载入，在原先的缓存预处理中，使用非padding的tokenizer对prompt进行处理后，
还使用了flip操作将得到的input_ids以及attention_mask进行了翻转倒序，并且在当时也提前解释了原因：主要是便于进行前侧padding的操作。
后续经过data_collator“倒序的数据经过padding后再flip回来”的操作，pad_token将会位于前侧，
而进行生成采样的actor将能接续prompt的内容进行自回归生成（更具体可参考【上篇】的0.3 板块问题 第1点）。

3.3.1.2 DataCollatorRLHF
正如上方所述，phase3使用的data_collator实例化自DataCollatorRLHF，
该类主要实现了“padding至max_prompt_len（默认为max_seq_len的一半），然后进行flip”。

。。。

3.3.2.2 DeepSpeedHybridEngine
这里实际上是本框架最具特色的部分，除了定制有更详尽的DeepSpeed配置来进行更精细的优化管理外，
DeepSpeed-Chat团队还专门为actor模型开发了一个名为“DeepSpeedHybridEngine”的优化引擎，正如其名“Hybrid（混合动力）”所述，
由于actor在PPO训练的过程中，需要兼任训练（参数优化）与复杂推理（生成经验序列seq），普通的优化引擎只能胜任单一的训练优化或单一的推理优化，
DeepSpeedHybridEngine将支持模型在两种模式中自动切换并享有相应优化，使得phase3的训练效率大幅提升，这也是DeepSpeed-Chat框架的优势所在。
009.png

3.3.3 根据prompt获取经验数据
...


3.3.5.4 EMA
待完善…

3.4 实例测试
“实例测试”与“指标评估”并不是完全相同的概念，实例测试是选择具体的数据实例输入进模型中，
人工观察其输出结果，而非使用具体指标对结果进行评估。
待完善…

3.5 相关拓展

3.5.1 phase3的参数设置

RLHF的训练涉及到强化学习，训练过程对超参数的设置极其敏感，DeepSpeed-Chat团队在尝试了多种参数设置后，
最终默认设置了per_device_train_batch_size(即prompt_batch_size) = per_device_mini_batch_size(即ppo_batch_size)，
且生成1个prompt_batch就立刻开始训练——这样一来，实际上在进行的就是On-Policy强化学习，采集一次、学习一次，数据利用率并不高。

此外，DeepSpeed-Chat团队还发现为无监督训练的损失设置系数（unsup_coef）也非常困难，
训练过程会变得更加震荡，不过团队也没有花费太多精力在调整这个系数参数上。

当然这些都并不是最佳的超参数配置，DeepSpeed-Chat团队仍鼓励用户多做尝试并分享出自己的调参经验。

We have found that it is very unstable to use different generation training batch sizes (–per_device_train_batch_size)
and PPO training batch sizes (–per_device_mini_batch_size), more than one PPO training epoch (–ppo_epochs),
or more than one generation batch size (–generation_batch_numbers).
These all point to the same problem:
we are not able to update the actor model multiple times after generating experimental data.
Therefore, in all of our successful runs,
we have set
per_device_train_batch_size = per_device_mini_batch_size and ppo_epochs = generation_batch_numbers = 1.
This is unexpected for a standard RL training pipeline, and we have tried different methods to overcome this,
but all have failed.
One of the most likely reasons for this instability is that we found the log_probs and
old_log_probs used in the actor_loss_fn function can quickly diverge even within two consecutive iterations,
which causes the corresponding ratio to be huge.
Setting a strict upper bound can alleviate this problem, but it cannot fully resolve the convergence issue.

We have also found that adding unsupervised training is not easy.
We tried using the coefficient (–unsup_coef=27.8) provided by InstructGPT,
but it caused instability in the RLHF training.
According to InstructGPT,
unsupervised training mainly affects the model quality on standard benchmarks instead of the RLHF performance.
We did not put much effort into tuning this parameter.

3.5.2 PPO-ptx训练的迭代数对齐
在phase3中，如果启用了无监督训练（PPO-ptx），那么无监督训练将是与PPO训练同步进行的，故两者的数据集处理几乎都是同步的，
不仅是batch_size相同，以至于两者的batch数（step数）也都会被强制持平：通常情况下无监督数据量更大，
按理同batch_size的情况下可迭代的次数也将多得多，但在PPO-ptx训练中，
无监督训练的迭代数将会被裁至与PPO训练所用数据的迭代数持平——例如PPO所用训练数据满足迭代10次、无监督训练也只能进行10次迭代，
多余的无监督数据将被弃用。

3.5.3 LMFlow的RAFT
  待完善…

3.6 版块相关问题
  暂无
————————————————
版权声明：本文为CSDN博主「Remixa」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/Remixa/article/details/130666878
InstructGPT高效实践——【DeepSpeed-Chat】源码详解(2/3)：Supervised Finetuning、Reward Model Finetuning

https://blog.csdn.net/Remixa/article/details/130655635?spm=1001.2014.3001.5501

目录
前言
1 phase-1: Supervised Finetuning
1.1 训练数据样例
1.2 训练过程
1.3 关键代码详解
1.3.1 基座模型结构
1.3.2 LoRA结构及其正向传播
1.3.3 phase1的指标评估
1.4 实例测试
1.5 相关拓展
1.5.1 多轮对话性能
1.5.2 本阶段训练更倾向过拟合
1.6 版块相关问题
2 phase-2: Reward Model Finetuning
2.1 训练数据样例
2.2 训练过程
2.3关键代码详解
2.3.1 RM具体结构
2.3.2 DataCollator及RM所需输入形式
2.3.3 RM的正向传播及成对排序损失
2.3.4 phase2的指标评估
2.4 实例测试
2.5 相关拓展
2.5.1 对话奖励聚合设计
2.6 板块相关问题
后续
前言
002.png
本篇为上中下三篇章的【中篇】，接续自【上篇】。主要针对三阶段训练中第一阶段、第二阶段较为重要的部分源码进行详解。
尽管官方的上手文档均是以sh文件为例进行演示，且源码中确实也提供了便捷的sh文件供直接使用，
但我仍建议通过各阶段的main.py文件（applications/DeepSpeed-Chat/training/step*/main.py）来运行训练，
大致原因有二：其一是因为官方预设的sh文件调用了applications/DeepSpeed-Chat/train.py，其中对模型选型等参数进行了严格的限制，
虽然提高了代码安全性，但对于以学习为目的的探索来说失去了一定的灵活性（见下方代码块），
直接通过main.py进行传参即可实现绕过限制、使用更轻量的模型进行训练；其二是因为main.py中的传参相对更接近底层，更有助于深入理解代码。

1 phase-1: Supervised Finetuning
1.1 训练数据样例
数据格式名称	        说明	                                                                        样例
chosen_sentence	    人类偏好的完整对话，由prompt衔接偏好应答chosen得到，适用于phase1和phase2。       	“Human: Please tell me about Microsoft in a few sentence? Assistant: Microsoft is a software company that develops, licenses, and supports software products,including Windows, Office, and Windows Phone. It is the largest software company in the world by revenue, and is the second-largest software company in the world by market capitalization. Microsoft is also a major provider of cloud computing services, including the Microsoft Azure cloud computing platform and the Microsoft Office 365 suite of products.”

模型将基于自回归语言建模任务对形如上述样例的数据进行训练，最后将得到具备更佳对话能力的模型SFT/ref(Supervised Fine-Tuning)。
更多的数据格式可见【上篇】的“1.2.1 数据格式基本概念”。

1.2 训练过程
    在此简单讲述UML时序图的元素含义：
    - 箭头表示信息传递：实线表示调用，虚线表示返回；
    - alt表示假设分支，其后方“[]”中的内容表示“条件”；
    - loop表示循环；
    - 淡蓝色区域即为高亮部分。

    applications/DeepSpeed-Chat/training/step1_supervised_finetuning/main.py
.....
yknote--TBD图太大，。。。。
phase1的大致训练过程如UML时序图所示（“括号序号”与UML时序图的“圈序号”对应）：

    1载入tokenizer(1-2)；
    2载入基座模型（目前仅支持部分CausalLM模型）(3-4)；
    3根据是否设置lora_dim（LoRA的低秩维度）判断是否启用LoRA技术，
    如果启用，则将基座模型结构进行LoRA改造（具体可见后续详述），并返回改造后的模型(5-6)；
    4判断是否启用“仅更新LoRA参数”，如果启用，则对其余结构参数进行冻结处理，并返回冻结处理后的模型(7-8)；
    5获取Dataset（具体流程可见【上篇】）(9-10)；
    6实例化DataLoader(11)；
    7使用DeepSpeed的优化技术DeepSpeedEngine包裹模型等对象(12)；
    8开始正式训练前首先进行指标评估，选用的指标为困惑度perplexity(13-14)；
    9开始训练，epoch循环：
        1step循环：
            1正向传播得到loss(15-18)，如果模型启用了LoRA技术，则正向传播还需要经过LoRA结构(16-17)；
            2反向传播计算梯度(19)；
            3更新模型参数（其中所涉及的梯度累计gradient_accumulation_steps将由DeepSpeedEngine自动进行管理，无需过度关注）(20)；
        2经过1个epoch的训练后进行指标评估(21-22)；
        3保存模型(23)。

1.3 关键代码详解
上述过程存在几个值得关注的地方（即文字描述加粗、UML时序图高亮的部分）：

    基座模型的基本结构，主要是观察其所使用的输出头类型，基本就能知道该阶段使用了什么样的模型进行训练；

    启用LoRA技术进行结构改造的细节及其正向传播过程；

    关于phase1的指标评估方式。

以下将对相关部分的源码进行讲解。

1.3.1 基座模型结构

....

1.3.2 LoRA结构及其正向传播
LoRA技术的大致思路如下图所示：
004.png

    1在关键的参数层中加入旁路；

    2原参数冻结不变，训练时优化旁路参数；

    3原路输出W x WxWx和旁路输出B A x BAxBAx的加和即为最终输出h = W x + B A x h=Wx+BAxh=Wx+BAx 。

LoRA结构定义
而DeepSpeed-Chat的实现基本与上述思路一致，当设置LoRA的低秩维度lora_dim（如lora_dim=128）时，
即认为启用了LoRA训练，则将原始模型中名称含有“deoder.layers.”且为线性层修改为LoRA层，
具体操作为：

    1将原始结构的weight参数冻结；
    2新引入了2个线性层lora_right_weight和lora_left_weight，可实现先降维至lora_dim再升维回原维度；
    3LoRA层主要实现了两分支通路，一条分支为已被冻结weight参数的原始结构、另一条分支为新引入的降维再升维线性层组。
。。。。
    # applications/DeepSpeed-Chat/training/step1_supervised_finetuning/main.py
    # 判断是否启用LoRA模式
    。。。。



1.4 实例测试
“实例测试”与“指标评估”并不是完全相同的概念，
实例测试是选择具体的数据实例输入进模型中，人工观察其输出结果，而非使用具体指标对结果进行评估。
实例测试实际上更体现了正向传播到解码、再到返回具体文本的过程。
例如我对模型输入了一句prompt文本，那么整个实例测试流将会返回给我一份answer文本，
我将以主观视角来感知这个answer的优劣程度，而不是采用具体的指标来进行评估。
待完善…

1.5 相关拓展
1.5.1 多轮对话性能
倘若想要使得模型通过具备更好的多轮对话的性能，除了需要考虑其“潜力”（就目前技术来说，模型支持的最大序列长度即为“潜力”，
不可否认未来是否会出现新的长文本拓展技术）外，其多轮对话性能表现仍主要与本阶段的训练数据有关，还需要为本阶段加入更多的多轮对话数据，
因此需要明确的是，用于本阶段的训练数据并不只局限于单轮对话，同样可以使用多轮对话内容进行训练，
多轮对话无非只是具有更长的prompt而已，单轮对话与多轮对话的数据样例可见下表。

单轮或多轮	            样例
单轮prompt	            "Human: Please tell me about Microsoft in a few sentence? Assistant: "
多轮prompt	            “Human: I’m buying a new video game console for the first time since in a decade, but I forget their setups. What do I need in addition to a Playstation 5 in order to play the console? Assistant: You need to buy a Playstation 5 first. Then you’ll also need a TV with HDMI input. It’s possible that HDMI to HDMI cables will also work, but it’s more likely that you’ll need a physical HDMI cord. You also might want to buy an extra power adapter, since the ones that come with new Playstation 5’s are quite short. Are you looking to play on a PC or any other system? That might affect what other hardware you need to buy. Human: Playstation 5’s cables aren’t short, but that’s good information. Can any television with an HDMI input play PS5? Assistant:”
单轮chosen_sentence	    “Human: Please tell me about Microsoft in a few sentence? Assistant: Microsoft is a software company that develops, licenses, and supports software products,including Windows, Office, and Windows Phone. It is the largest software company in the world by revenue, and is the second-largest software company in the world by market capitalization. Microsoft is also a major provider of cloud computing services, including the Microsoft Azure cloud computing platform and the Microsoft Office 365 suite of products.”
多轮chosen_setence	    “Human: I’m buying a new video game console for the first time since in a decade, but I forget their setups. What do I need in addition to a Playstation 5 in order to play the console? Assistant: You need to buy a Playstation 5 first. Then you’ll also need a TV with HDMI input. It’s possible that HDMI to HDMI cables will also work, but it’s more likely that you’ll need a physical HDMI cord. You also might want to buy an extra power adapter, since the ones that come with new Playstation 5’s are quite short. Are you looking to play on a PC or any other system? That might affect what other hardware you need to buy. Human: Playstation 5’s cables aren’t short, but that’s good information. Can any television with an HDMI input play PS5? Assistant: So you’ve got a Playstation 5 and a TV that you’re going to connect together with an HDMI cable, and you want to know if that’s going to work? It’s definitely possible for the two to work together, and you might need an additional power adapter if your TV only came with a shorter adapter. However, it may be difficult to determine if it will work for sure. This is one area where troubleshooting and making educated guesses may be necessary. You should still be able to easily use your console, but it may be necessary to troubleshoot first.”

1.5.2 本阶段训练更倾向过拟合
DeepSpeed-Chat团队称，根据InstructGPT的建议，本阶段的训练结果应适当倾向于过拟合（可以考虑更多的epoch），以此获得更好的对话能力。
DeepSpeed-Chat团队还发现这个设计尤其对诸如opt-1.3B这类较小的模型微调特别有效。

From InstructGPT work, it is recommended to train the model for overfitting (aka longer epochs) for better human-preferred answers.
Through our exploration, we have found this to be particularly helpful for smaller model finetuning, such as OPT-1.3B.

1.6 版块相关问题
暂无

2 phase-2: Reward Model Finetuning

2.1 训练数据样例
数据格式名称	        说明	                                                                            样例
chosen_sentence	    人类偏好的完整对话，由prompt衔接偏好应答chosen得到，适用于phase1和phase2。	        “Human: Please tell me about Microsoft in a few sentence? Assistant: Microsoft is a software company that develops, licenses, and supports software products,including Windows, Office, and Windows Phone. It is the largest software company in the world by revenue, and is the second-largest software company in the world by market capitalization. Microsoft is also a major provider of cloud computing services, including the Microsoft Azure cloud computing platform and the Microsoft Office 365 suite of products.”
reject_sentence	    人类排斥的完整对话，由prompt衔接排斥应答rejected得到，适用于phase2。	            “Human: Please tell me about Microsoft in a few sentence? Assistant: I’m not sure what you mean.”

模型将基于排序损失对形如上述样例的数据对进行训练，最后将得到具备类人评分能力的RM(Reward Model)。
更多的数据格式可见【上篇】的“1.2.1 数据格式基本概念”。

2.2 训练过程
    在此简单讲述UML时序图的元素含义：
    - 箭头表示信息传递：实线表示调用，虚线表示返回；
    - alt表示假设分支，其后方“[]”中的内容表示“条件”；
    - loop表示循环；
    - 淡蓝色区域即为高亮部分。

yknote---TBD截图

phase2的大致训练过程如UML时序图所示（“括号序号”与UML时序图的“圈序号”对应）：

1载入tokenizer(1-2);
2载入模型（rm_model），其中涉及一定的结构更改(3-8)；
3根据是否设置lora_dim（LoRA的低秩维度）判断是否启用LoRA技术，
 如果启用，则将基座模型结构进行LoRA改造（具体可见后续详述），并返回改造后的模型(9-10)；
4判断是否启用“仅更新LoRA参数”，如果启用，则对其余结构参数进行冻结处理，并返回冻结处理后的模型(11-12)；
5获取Dataset（具体流程可见【上篇】）(13-14)；
6实例化DataCollator，用于进一步对加载的数据进行整理(15-16)；
7实例化DataLoader(17)；
8使用DeepSpeed的优化技术DeepSpeedEngine包裹rm_model等对象(18)；
9开始正式训练前首先进行指标评估，选用的指标为排序结果的准确率accuracy(19-20)；
10.开始训练，epoch循环：
    1step循环：
        1正向传播得到loss(21-26)，
         如果模型启用了LoRA技术，则正向传播还需要经过LoRA结构(23-24)；
        2反向传播计算梯度(27)；
        3更新模型参数（其中所涉及的梯度累计gradient_accumulation_steps将由DeepSpeedEngine自动进行管理，
         无需过度关注）(28)；
    3经过1个epoch的训练后进行指标评估(29-30)；
    3保存模型(31)。

2.3关键代码详解
上述过程存在几个值得关注的地方（即文字描述加粗、UML时序图高亮的部分）：

    rm_model(RM)的具体结构；

    phase2的数据整理器DataCollatorReward所实现的操作，通过这部分可以了解rm_model所需的输入形式；

    关于phase2的指标评估方式；

    rm_model的正向传播过程。

。。。。。

2.3.3 RM的正向传播及成对排序损失
RM的正向传播过程不算复杂，总的来说就是：

    1 数据经过主干网络得到shape为(bs*2, max_seq_len, hidden_size)的最后层输出特征hidden_states；

    2 然后将输出特征送入线性层v_head得到shape为(bs*2, max_seq_len)的评分rewards。

较为复杂的部分实际上是“成对排序损失的计算”以及“评分聚合设计”。
007.png
成对排序损失（Pairwise Ranking Loss）
该损失函数的目的在于最大化“chosen/好的/排序靠前的”和“rejected/坏的/排序靠后的”的差值，由此促使r_θ学习到相应的排序模式。
DeepSpeed-Chat在实现这部分时，r_θ(x , y_c)和r_θ(x, y_r)分别选择了chosen_sentence和reject_sentence两者answer的对齐部分，
通过文字叙述略显抽象，查看下方的代码块有助于你理解这个概念：

    max_seq_len为10，pad_token_id为0，
    有同属同个prompt的chosen_sentence和reject_sentence:
    prompt: [11, 22, 33]
    chosen_sentence: [11, 22, 33, 44, 55, 66, 0, 0, 0, 0]
    reject_sentence: [11, 22, 33, 40, 50, 0, 0, 0, 0, 0]

    “两者answer的对齐部分”即为“非prompt部分也非padding部分、但长度要对齐”：
    chosen_truncated: [44, 55, 66]
    reject_truncated: [40, 50, 0]

    chosen_sentence的answer比较长，所以reject_sentence在取相应部分时要取至与chosen部分等长为止；
    reject_sentence的answer较长时同理。

为了取到上述提及的“对齐部分”，代码进行了较为晦涩抽象的取index操作，
但只要理解其最终目的是为了取到chosen_sentence和reject_sentence对齐部分的reward，来进行损失计算即可。

对话奖励设计
尽管使用的是“对齐部分”的reward来计算成对排序损失，
但RM模型对一个对话的预测评分实际上取的是该对话文本最后一个有效token（通常会是“结束标记”）的reward，
下方代码块提供了一个简单例子说明了这个情况。

    pad_token_id = 0
    conversation = [11, 22, 33, 44, 55, 66, 0, 0, 0, 0]
    conversation_rewards = [2.01, 0.23, 2.89, 0.66, 0.33, 2.25, 0.36, 0.99, 1.32, 1.62]
    token_id为66的token作为该对话的最后1个有效token，
    其对应的reward“2.25”将被用于表示整个对话的reward。

整体代码如下所示：

# applications/DeepSpeed-Chat/training/utils/model/reward_model.py
class RewardModel(nn.Module):
	def __init__(self, ···):
		···



2.4 实例测试
“实例测试”与“指标评估”并不是完全相同的概念，实例测试是选择具体的数据实例输入进模型中，人工观察其输出结果，而非使用具体指标对结果进行评估。
待完善…

2.5 相关拓展
2.5.1 对话奖励聚合设计
在DeepSpeed-Chat的实现中，RM模型对一个对话的预测评分实际上取的是该对话文本最后一个token的reward，
当然此处并不是只能采用这种方式对对话进行评分，这是一个开放性的策略设计，只是DeepSpeed-Chat团队采取了这样的实现，
用户当然也可以自己制定评分的处理策略，比如answer部分的平均reward、序列reward再接全连接层得到聚合rewad等等。

In our implementation,
we use either the end token of the sequence or the first padding token as the aggregated score and compare them.
Others may also use the average score for the entire answer as an alternative.

2.6 板块相关问题
暂无

后续
RLHF阶段的训练具体内容可见【下篇】。
RLHF中的PPO算法原理及其实现

https://wjn1996.blog.csdn.net/article/details/130814286

RLHF中的PPO算法原理及其实现
ChatGPT是基于InstructGPT实现的多轮对话生成式大模型。ChatGPT主要涉及到的技术包括：

    指令微调（Instruction-tuning）；

    因果推断（Causal Language Modeling）；

    人类对齐（Human Alignment）

博主在之前的文章中已经介绍过关于指令微调以及相关Prompting技术的原理
（可以详见：Prompt-Tuning——深度解读一种新的微调范式）以及关于GPT等因果语言模型的相关介绍：
【预训练语言模型】GPT: Improving Language Understanding by Generative Pre-Training。
那么除了如何训练一个基本的生成式模型外，大模型还需要关注于如何让生成式大模型更加符合人类价值观。

在之前的文章InstructGPT原理讲解及ChatGPT类开源项目中已经介绍了ChatGPT以及最近开源的一些类ChatGPT模型是如何实现对齐的，
这里我们也详细介绍一下InstructGPT中进行人类对齐的核心算法——RLHF（人类对齐的强化学习）PPO算法。

本篇文章主要参考下面两个参考资料：
【1】强化学习极简入门：通俗理解MDP、DP MC TC和Q学习、策略梯度、PPO
【2】基于DeepSpeed训练ChatGPT

一、RLHF PPO 算法原理
PPO算法是一种具体的Actor-Critic算法实现，比如在对话机器人中，输入的prompt是state，输出的response是action，
想要得到的策略就是怎么从prompt生成action能够得到最大的reward，也就是拟合人类的偏好。

PPO算法涉及到两个策略：

    近端策略优化惩罚（PPO-penalty）；

    近端策略优化裁剪PPO-clip。

重要性采样
因为在Actor-Critic训练时，策略函数参数进行优化后，上一轮策略采样的动作-状态序列就不能用了，
因此需要进行重要性采样，来避免每次更新策略函数后的重复采样问题。
当不能在分布p中采样数据，而只能从另外一个分布q中去采样数据时(q可以是任何分布）。

重要性采样的原理：  020.png

KL散度约束：
重要性采样中，p和q分布不能查得太远，所以需要有KL散度施加约束。

Advantage：
Actor-Critic算法中，需要定义advantage，最简单的就是定义Reward-baseline，也可以定义为。
其中Vπ( s )可以理解为当前状态 s 下所有动作执行后得到的奖励的期望，而 Qπ(s , a)表示当前状态s下指定某一个动作a得到的奖励。
所以如果 Aπ( s , a ) > 0 ，则说明当前动作 a 所获的奖励是大于整体期望的，所以应该极大化这个动作的概率。

总的来说，Advantage旨在通过正负值来告诉策略什么动作可以是可以得到正反馈，避免仅有Reward作为绝对值时所带来的高方差问题。

Advantage+重要性采样：
Advantage可以认为是重要性采样中的f(x)。由于其在优化过程中参数是在变的，所以需要进行重要性采样，因此优化目标变为：
022.png  yknote截图
近端策略优化惩罚（PPO-penalty）
PPO算法之近端策略优化惩罚的原理如下图所示：
021.png

近端策略优化裁剪PPO-clip
优化目标改为下面：
023.png
公式的理解：
024.png
所以说，clip本质上也是约束两个分布不要差的太远，
其相比KL散度来说，KL散度是在两个分布的输出logits上进行约束，而clip方法则是直接在概率比上做约束。


二、RLHF PPO算法实现
（1）首先初始化RLHF类和PPOTrainer

    rlhf_engine = DeepSpeedRLHFEngine(
            actor_model_name_or_path=args.actor_model_name_or_path,
            critic_model_name_or_path=args.critic_model_name_or_path,
            tokenizer=tokenizer,
            num_total_iters=num_total_iters,
            args=args)
    ppo_trainer =  DeepSpeedPPOTrainer
    trainer = ppo_trainer(rlhf_engine, args)

初始化过程中，加载4个模型，包括Actor、SFT、Critic和Reward：

代码中的self.ref其实就是SFT模型

class DeepSpeedRLHFEngine():
    def __init__(self, actor_model_name_or_path, critic_model_name_or_path,
                 tokenizer, args, num_total_iters):
。。。

（2）加载用于RLHF的训练数据

    prompt_train_dataloader, num_total_iters = create_datasets(
            args=args, tokenizer=tokenizer, train_phase=3)

（3）RLHF整体训练过程如下
具体的流程可以详看代码注释，总的来说，主要流程包括：

    遍历每个epoch，每个epoch里遍历每个batch；
    对于每个batch，先采样一堆经验数据；
    根据经验数据，训练Actor和Critic模型

# 训练的总Epoch数
for epoch in range(args.num_train_epochs):
    # 遍历每一个Batch

这个训练过程主要包括两个核心步骤：

    采样Experience数据；

    根据采样的数据训练Actor和Critic模型。

下面详细分析一下这两个核心步骤，理解了这两个核心步骤也就差不多理解了RLHF PPO算法了。

Experience采样
025.png
图来自这里。

实现细节详见代码及注释：

    def generate_experience(self, prompts):
        self.eval() # 开启eval模式

获得Advantage，并更新Actor和Critic参数
026.png

def train_rlhf(self, inputs):
	# 当前RLHF轮次最初采样的经验池中采样一批数据

博主会不断更新关于大模型方面更多技术，相关文章请见：

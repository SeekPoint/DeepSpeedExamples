ub2004@ub2004-B85M-A0:~/yk_fork/DeepSpeedExamples/training/cifar$ python3 cifar10_deepspeed.py --deepspeed_config ds_config.json
[2023-07-30 11:49:33,337] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
cifar10ds pid: 266134 at ub2004-B85M-A0 File: /home/ub2004/yk_fork/DeepSpeedExamples/training/cifar/cifar10_deepspeed.py Func: <module> Line#: 98 Info: ds init start
[2023-07-30 11:49:34,716] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-07-30 11:49:34,716] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-07-30 11:49:34,716] [INFO] [comm.py:627:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-07-30 11:49:34,755] [INFO] [comm.py:677:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=192.168.1.7, master_port=29500
[2023-07-30 11:49:34,755] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
cifar10ds pid: 266134 at ub2004-B85M-A0 File: /home/ub2004/yk_fork/DeepSpeedExamples/training/cifar/cifar10_deepspeed.py Func: <module> Line#: 100 Info: ds init start
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170498071/170498071 [00:16<00:00, 10399821.94it/s]
Extracting ./data/cifar-10-python.tar.gz to ./data
Files already downloaded and verified
 frog   car  bird  deer
[2023-07-30 11:49:57,868] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-07-30 11:49:57,869] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[2023-07-30 11:49:57,896] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: True
Using /home/ub2004/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Creating extension directory /home/ub2004/.cache/torch_extensions/py310_cu117/fused_adam...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/ub2004/.cache/torch_extensions/py310_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/ub2004/.local/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/home/ub2004/.local/lib/python3.10/site-packages/deepspeed/ops/csrc/adam -isystem /home/ub2004/.local/lib/python3.10/site-packages/torch/include -isystem /home/ub2004/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/ub2004/.local/lib/python3.10/site-packages/torch/include/TH -isystem /home/ub2004/.local/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 -std=c++14 -c /home/ub2004/.local/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/ub2004/.local/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/home/ub2004/.local/lib/python3.10/site-packages/deepspeed/ops/csrc/adam -isystem /home/ub2004/.local/lib/python3.10/site-packages/torch/include -isystem /home/ub2004/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/ub2004/.local/lib/python3.10/site-packages/torch/include/TH -isystem /home/ub2004/.local/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /home/ub2004/.local/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/home/ub2004/.local/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so
Loading extension module fused_adam...
Time to load fused_adam op: 24.40533423423767 seconds
[2023-07-30 11:50:22,580] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-07-30 11:50:22,581] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-07-30 11:50:22,581] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-07-30 11:50:22,582] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-07-30 11:50:22,582] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2023-07-30 11:50:22,582] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fec1c36af80>
[2023-07-30 11:50:22,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[[0.8, 0.999]]
[2023-07-30 11:50:22,582] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-07-30 11:50:22,582] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-07-30 11:50:22,582] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-07-30 11:50:22,582] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-07-30 11:50:22,582] [INFO] [config.py:964:print]   amp_params ................... False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fec1c36b490>
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   dump_state ................... False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 32768, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": true, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-07-30 11:50:22,583] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   gradient_clipping ............ 1
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 32768
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   pld_params ................... False
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   scheduler_name ............... WarmupLR
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 1000}
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   steps_per_print .............. 2000
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   train_batch_size ............. 16
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  16
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   wall_clock_breakdown ......... True
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   world_size ................... 1
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=50000000 allgather_partitions=True allgather_bucket_size=50000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-07-30 11:50:22,584] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-07-30 11:50:22,585] [INFO] [config.py:950:print_user_config]   json = {
    "flops_profiler": {
        "enabled": true, 
        "profile_step": 1, 
        "module_depth": -1, 
        "top_modules": 1, 
        "detailed": true, 
        "output_file": null
    }, 
    "train_batch_size": 16, 
    "steps_per_print": 2.000000e+03, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.001, 
            "betas": [0.8, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 3e-07
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 0, 
            "warmup_max_lr": 0.001, 
            "warmup_num_steps": 1000
        }
    }, 
    "gradient_clipping": 1, 
    "prescale_gradients": false, 
    "fp16": {
        "enabled": true, 
        "fp16_master_weights_and_grads": false, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 15
    }, 
    "wall_clock_breakdown": false, 
    "zero_optimization": {
        "stage": 0, 
        "allgather_partitions": true, 
        "reduce_scatter": true, 
        "allgather_bucket_size": 5.000000e+07, 
        "reduce_bucket_size": 5.000000e+07, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "cpu_offload": false
    }
}
fp16=True
[2023-07-30 11:50:23,372] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | compute_norm: 0.29 | overflow_check: 0.89 | unscale_and_clip: 0.07 | basic_step: 0.52 | update_fp16: 0.22
[2023-07-30 11:50:23,372] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 672.06 | backward_microstep: 51.89 | backward_inner_microstep: 51.73 | backward_allreduce_microstep: 0.13 | step_microstep: 2.89
[2023-07-30 11:50:23,372] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 672.06 | backward: 51.88 | backward_inner: 51.72 | backward_allreduce: 0.13 | step: 2.89
[2023-07-30 11:50:23,382] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | compute_norm: 0.18 | overflow_check: 0.87 | unscale_and_clip: 0.04 | basic_step: 0.35 | update_fp16: 0.22

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 2:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   1       
data parallel size:                                           1       
model parallel size:                                          1       
batch size per GPU:                                           16      
params per gpu:                                               62.01 k 
params of model = params per GPU * mp_size:                   62.01 k 
fwd MACs per GPU:                                             10.43 MMACs
fwd flops per GPU:                                            21.16 M 
fwd flops of model = fwd flops per GPU * mp_size:             21.16 M 
fwd latency:                                                  2.06 ms 
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          10.26 GFLOPS
bwd latency:                                                  3.32 ms 
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    12.73 GFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   11.78 GFLOPS
step latency:                                                 2.21 ms 
iter latency:                                                 7.6 ms  
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       8.35 GFLOPS
samples/second:                                               2105.51 

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'Net': '62.01 k'}
    MACs        - {'Net': '10.43 MMACs'}
    fwd latency - {'Net': '2.01 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

Net(
  62.01 k, 100.00% Params, 10.43 MMACs, 100.00% MACs, 2.01 ms, 100.00% latency, 10.53 GFLOPS, 
  (conv1): Conv2d(456, 0.74% Params, 5.64 MMACs, 54.13% MACs, 683.55 us, 34.01% latency, 16.63 GFLOPS, 3, 6, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(0, 0.00% Params, 0 MACs, 0.00% MACs, 181.67 us, 9.04% latency, 555.19 MFLOPS, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(2.42 k, 3.90% Params, 3.84 MMACs, 36.83% MACs, 424.15 us, 21.10% latency, 18.17 GFLOPS, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(48.12 k, 77.61% Params, 768.0 KMACs, 7.37% MACs, 199.79 us, 9.94% latency, 7.69 GFLOPS, in_features=400, out_features=120, bias=True)
  (fc2): Linear(10.16 k, 16.39% Params, 161.28 KMACs, 1.55% MACs, 98.23 us, 4.89% latency, 3.28 GFLOPS, in_features=120, out_features=84, bias=True)
  (fc3): Linear(850, 1.37% Params, 13.44 KMACs, 0.13% MACs, 74.63 us, 3.71% latency, 360.2 MFLOPS, in_features=84, out_features=10, bias=True)
)
------------------------------------------------------------------------------
[2023-07-30 11:50:23,383] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2.05 | backward_microstep: 3.32 | backward_inner_microstep: 3.19 | backward_allreduce_microstep: 0.12 | step_microstep: 2.21
[2023-07-30 11:50:23,383] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 0.00 | backward: 0.00 | backward_inner: 3.19 | backward_allreduce: 0.12 | step: 0.00
[2023-07-30 11:50:23,390] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | compute_norm: 0.18 | overflow_check: 0.90 | unscale_and_clip: 0.04 | basic_step: 0.35 | update_fp16: 0.22
[2023-07-30 11:50:23,390] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1.28 | backward_microstep: 3.17 | backward_inner_microstep: 3.04 | backward_allreduce_microstep: 0.12 | step_microstep: 2.27
[2023-07-30 11:50:23,391] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 1.27 | backward: 3.17 | backward_inner: 3.04 | backward_allreduce: 0.12 | step: 2.27
[2023-07-30 11:50:23,398] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | compute_norm: 0.18 | overflow_check: 0.91 | unscale_and_clip: 0.05 | basic_step: 0.35 | update_fp16: 0.22
[2023-07-30 11:50:23,398] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1.29 | backward_microstep: 3.18 | backward_inner_microstep: 3.04 | backward_allreduce_microstep: 0.12 | step_microstep: 2.29
[2023-07-30 11:50:23,398] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 1.28 | backward: 3.18 | backward_inner: 3.04 | backward_allreduce: 0.12 | step: 2.29
[2023-07-30 11:50:23,405] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | compute_norm: 0.18 | overflow_check: 0.96 | unscale_and_clip: 0.04 | basic_step: 0.35 | update_fp16: 0.22
[2023-07-30 11:50:23,405] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1.29 | backward_microstep: 3.19 | backward_inner_microstep: 3.05 | backward_allreduce_microstep: 0.12 | step_microstep: 2.31
[2023-07-30 11:50:23,406] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 1.28 | backward: 3.19 | backward_inner: 3.05 | backward_allreduce: 0.12 | step: 2.32
[2023-07-30 11:50:23,413] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | compute_norm: 0.18 | overflow_check: 0.88 | unscale_and_clip: 0.04 | basic_step: 0.34 | update_fp16: 0.22
[2023-07-30 11:50:23,413] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1.30 | backward_microstep: 3.20 | backward_inner_microstep: 3.06 | backward_allreduce_microstep: 0.12 | step_microstep: 2.25
[2023-07-30 11:50:23,413] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 1.28 | backward: 3.20 | backward_inner: 3.06 | backward_allreduce: 0.12 | step: 2.25
[2023-07-30 11:50:23,420] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | compute_norm: 0.18 | overflow_check: 0.88 | unscale_and_clip: 0.04 | basic_step: 0.35 | update_fp16: 0.22
[2023-07-30 11:50:23,420] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1.30 | backward_microstep: 3.17 | backward_inner_microstep: 3.04 | backward_allreduce_microstep: 0.12 | step_microstep: 2.22
[2023-07-30 11:50:23,420] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 1.29 | backward: 3.17 | backward_inner: 3.04 | backward_allreduce: 0.12 | step: 2.22
[2023-07-30 11:50:23,427] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | compute_norm: 0.17 | overflow_check: 0.89 | unscale_and_clip: 0.04 | basic_step: 0.34 | update_fp16: 0.22
[2023-07-30 11:50:23,428] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1.30 | backward_microstep: 3.17 | backward_inner_microstep: 3.04 | backward_allreduce_microstep: 0.12 | step_microstep: 2.23
[2023-07-30 11:50:23,428] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 1.28 | backward: 3.18 | backward_inner: 3.04 | backward_allreduce: 0.12 | step: 2.23
[2023-07-30 11:50:23,435] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | compute_norm: 0.18 | overflow_check: 0.88 | unscale_and_clip: 0.04 | basic_step: 0.34 | update_fp16: 0.22
[2023-07-30 11:50:23,435] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1.31 | backward_microstep: 3.17 | backward_inner_microstep: 3.04 | backward_allreduce_microstep: 0.12 | step_microstep: 2.23




。。。。。。。。。。。。。。。。。。。。。。。。。。。。。


ep_microstep: 2.11
[2023-07-30 11:52:49,620] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 1.27 | backward: 3.17 | backward_inner: 3.03 | backward_allreduce: 0.12 | step: 2.11
[2023-07-30 11:52:49,627] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | compute_norm: 0.17 | overflow_check: 0.86 | unscale_and_clip: 0.04 | basic_step: 0.33 | update_fp16: 0.22
[2023-07-30 11:52:49,627] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1.27 | backward_microstep: 3.18 | backward_inner_microstep: 3.05 | backward_allreduce_microstep: 0.12 | step_microstep: 2.11
[2023-07-30 11:52:49,627] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 1.26 | backward: 3.18 | backward_inner: 3.05 | backward_allreduce: 0.12 | step: 2.11
Finished Training
GroundTruth:    cat  ship  ship plane
Predicted:    cat   car  ship plane
Accuracy of the network on the 10000 test images: 55 %
Accuracy of plane : 65 %
Accuracy of   car : 70 %
Accuracy of  bird : 43 %
Accuracy of   cat : 32 %
Accuracy of  deer : 41 %
Accuracy of   dog : 47 %
Accuracy of  frog : 60 %
Accuracy of horse : 67 %
Accuracy of  ship : 77 %
Accuracy of truck : 46 %
ub2004@ub2004-B85M-A0:~/yk_fork/DeepSpeedExamples/training/cifar$ 
